{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bdc562a3",
      "metadata": {
        "id": "bdc562a3"
      },
      "source": [
        "# Trabajo Integrador Individual - NLP  \n",
        "## \"De Texto Crudo a Insights: Construyendo un pipeline reproducible de PLN\"\n",
        "\n",
        "> **Nota**: Este cuaderno es una **plantilla lista para usar**. Solo necesitás poner tus archivos `.txt` en la carpeta `corpus/` (una por documento) y completar `metadata.csv` en la raíz con la información de cada archivo.\n",
        "\n",
        "---\n",
        "\n",
        "### Estructura esperada\n",
        "```\n",
        "/corpus/\n",
        "  texto_1.txt\n",
        "  texto_2.txt\n",
        "  ...\n",
        "metadata.csv\n",
        "Trabajo_Integrador_NLP_Base.ipynb\n",
        "```\n",
        "\n",
        "### Requisitos técnicos\n",
        "- Python 3.9+\n",
        "- Librerías: `pandas`, `numpy`, `scikit-learn`, `nltk`, `spacy`, `gensim`, `pyLDAvis`, `matplotlib`, `wordcloud`\n",
        "- Modelo spaCy para español (opcional, recomendado): `es_core_news_sm`\n",
        "\n",
        "> Si no tenés algunas librerías, ejecutá la celda de instalación que sigue.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bec507fa",
      "metadata": {
        "id": "bec507fa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SpanishStemmer\n",
        "\n",
        "USE_SPACY = True\n",
        "try:\n",
        "    import spacy\n",
        "    try:\n",
        "        nlp = spacy.load(\"es_core_news_sm\")\n",
        "    except Exception as e:\n",
        "        print(\"No se encontró el modelo 'es_core_news_sm'. Se usará stemming como alternativa.\")\n",
        "        USE_SPACY = False\n",
        "except Exception as e:\n",
        "    print(\"spaCy no disponible. Se usará stemming como alternativa.\")\n",
        "    USE_SPACY = False\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora, models\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "SPANISH_STOPS = set(stopwords.words('spanish'))\n",
        "CUSTOM_STOPS = set()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82f051d7",
      "metadata": {
        "id": "82f051d7"
      },
      "source": [
        "## Sección 1: Presentación del Corpus (15%)\n",
        "\n",
        "En esta sección, describí **qué corpus elegiste y por qué**. Explicá criterios de selección, tamaño, fuente, y mostrás estadísticas básicas.\n",
        "\n",
        "**Checklist**:\n",
        "- [ ] Justificación del dominio elegido\n",
        "- [ ] Cantidad de documentos y palabras\n",
        "- [ ] Distribución por categorías/fechas (si aplica)\n",
        "- [ ] Muestras de texto\n",
        "- [ ] Nube de palabras (opcional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fdeb335",
      "metadata": {
        "id": "7fdeb335"
      },
      "outputs": [],
      "source": [
        "# Cargar metadata\n",
        "metadata_path = \"metadata.csv\"\n",
        "md = pd.read_csv(metadata_path, encoding=\"utf-8\")\n",
        "print(\"Registros en metadata:\", len(md))\n",
        "md.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd581f22",
      "metadata": {
        "id": "bd581f22"
      },
      "outputs": [],
      "source": [
        "# Cargar textos del directorio 'corpus/'\n",
        "def load_texts_from_folder(folder='corpus'):\n",
        "    files = sorted(glob.glob(os.path.join(folder, \"*.txt\")))\n",
        "    docs = []\n",
        "    for fp in files:\n",
        "        with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            text = f.read()\n",
        "        docs.append({\"archivo\": os.path.basename(fp), \"texto\": text})\n",
        "    return pd.DataFrame(docs)\n",
        "\n",
        "docs_df = load_texts_from_folder('corpus')\n",
        "print(\"Documentos cargados:\", len(docs_df))\n",
        "docs_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3562fe1",
      "metadata": {
        "id": "d3562fe1"
      },
      "outputs": [],
      "source": [
        "# Unir metadata con textos (por 'archivo')\n",
        "data = md.merge(docs_df, on='archivo', how='inner')\n",
        "print(\"Registros luego del merge:\", len(data))\n",
        "\n",
        "# Completar conteo de palabras si falta\n",
        "def word_count(txt):\n",
        "    return len(re.findall(r\"\\b\\w+\\b\", str(txt), flags=re.UNICODE))\n",
        "\n",
        "if \"palabras\" not in data.columns:\n",
        "    data[\"palabras\"] = data[\"texto\"].apply(word_count)\n",
        "else:\n",
        "    data[\"palabras\"] = data[\"texto\"].apply(word_count)\n",
        "\n",
        "display_cols = [c for c in [\"archivo\",\"titulo\",\"autor\",\"fecha\",\"categoria\",\"palabras\"] if c in data.columns]\n",
        "data[display_cols].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f486a05",
      "metadata": {
        "id": "1f486a05"
      },
      "outputs": [],
      "source": [
        "# Estadística Descriptiva\n",
        "print(\"Cantidad de documentos:\", len(data))\n",
        "print(\"Total de palabras:\", int(data['palabras'].sum()))\n",
        "print(\"Promedio de palabras por doc:\", round(data['palabras'].mean(), 2))\n",
        "\n",
        "# Distribución por categoría (si existe)\n",
        "if \"categoria\" in data.columns:\n",
        "    print(\"\\nDistribución por categoría:\")\n",
        "    print(data['categoria'].value_counts())\n",
        "\n",
        "# Distribución temporal (si existe 'fecha')\n",
        "if \"fecha\" in data.columns:\n",
        "    try:\n",
        "        data['fecha'] = pd.to_datetime(data['fecha'], errors='coerce')\n",
        "        by_year = data.dropna(subset=['fecha']).groupby(data['fecha'].dt.year).size()\n",
        "        print(\"\\nDocumentos por año:\")\n",
        "        print(by_year)\n",
        "    except Exception as e:\n",
        "        print(\"No se pudo parsear 'fecha':\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7496d0a",
      "metadata": {
        "id": "c7496d0a"
      },
      "outputs": [],
      "source": [
        "# Muestras de texto\n",
        "for i, row in data.head(3).iterrows():\n",
        "    print(f\"\\n=== {row.get('titulo', row['archivo'])} ===\")\n",
        "    print(str(row['texto'])[:500], '...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf35b71",
      "metadata": {
        "id": "5cf35b71"
      },
      "outputs": [],
      "source": [
        "# Se muestra nube de palabras con todos los textos concatenados\n",
        "all_text = \" \".join(data['texto'].astype(str).tolist()) if len(data) else \"\"\n",
        "if all_text.strip():\n",
        "    wc = WordCloud(width=900, height=500, background_color=\"white\").generate(all_text)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Nube de Palabras - Corpus completo\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No hay texto para generar la nube de palabras.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22b1b50f",
      "metadata": {
        "id": "22b1b50f"
      },
      "source": [
        "## Sección 2: Preprocesamiento (20%)\n",
        "\n",
        "Pasos típicos: minúsculas, limpieza, tokenización, stopwords, lematización/stemming.  \n",
        "Guardaremos también una **versión limpia** del texto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a89e05b0",
      "metadata": {
        "id": "a89e05b0"
      },
      "outputs": [],
      "source": [
        "# Funciones de limpieza y normalización\n",
        "LOWERCASE = True\n",
        "REMOVE_NUMBERS = False      # activalo si tus números no aportan significado\n",
        "REMOVE_PUNCT = True\n",
        "MIN_TOKEN_LEN = 2\n",
        "\n",
        "# se agrega stopwords propias si tu dominio lo requiere\n",
        "CUSTOM_STOPS = set([\n",
        "    # \"argentina\",\"años\",\"cosas\"\n",
        "])\n",
        "\n",
        "PUNCT_RE = re.compile(r\"[^\\wáéíóúñüÁÉÍÓÚÑÜ#@]+\", flags=re.UNICODE)\n",
        "\n",
        "stemmer = SpanishStemmer()\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    # Tokenización simple por palabras\n",
        "    return nltk.word_tokenize(text, language=\"spanish\")\n",
        "\n",
        "def normalize_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    if LOWERCASE:\n",
        "        text = text.lower()\n",
        "    if REMOVE_PUNCT:\n",
        "        text = PUNCT_RE.sub(\" \", text)\n",
        "    if not REMOVE_NUMBERS:\n",
        "        # se mantiene números (se podria quitar, sino activar REMOVE_NUMBERS)\n",
        "        pass\n",
        "    return text\n",
        "\n",
        "def lemmatize_spacy(tokens):\n",
        "    if not USE_SPACY:\n",
        "        return tokens\n",
        "    doc = nlp(\" \".join(tokens))\n",
        "    return [t.lemma_ for t in doc]\n",
        "\n",
        "def stem_spanish(tokens):\n",
        "    return [stemmer.stem(t) for t in tokens]\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    stops = SPANISH_STOPS.union(CUSTOM_STOPS)\n",
        "    return [t for t in tokens if t not in stops and len(t) >= MIN_TOKEN_LEN and not t.isspace()]\n",
        "\n",
        "def preprocess_pipeline(text, use_lemmatize=True):\n",
        "    text = normalize_text(text)\n",
        "    toks = simple_tokenize(text)\n",
        "    toks = remove_stopwords(toks)\n",
        "    if use_lemmatize and USE_SPACY:\n",
        "        toks = lemmatize_spacy(toks)\n",
        "        toks = remove_stopwords(toks)  # quitar stopwords que aparezcan tras lematizar\n",
        "    else:\n",
        "        toks = stem_spanish(toks)\n",
        "    return toks\n",
        "\n",
        "# Ejecutar el preprocesamiento\n",
        "data['tokens'] = data['texto'].astype(str).apply(lambda t: preprocess_pipeline(t, use_lemmatize=True))\n",
        "data['texto_limpio'] = data['tokens'].apply(lambda ts: \" \".join(ts))\n",
        "\n",
        "data[['archivo','palabras','texto_limpio']].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42c9e71f",
      "metadata": {
        "id": "42c9e71f"
      },
      "outputs": [],
      "source": [
        "# Comparación rápida ex ante y post (longitudes y vocabulario)\n",
        "orig_wc = data['texto'].apply(lambda t: len(re.findall(r\"\\b\\w+\\b\", str(t)))).sum()\n",
        "clean_wc = data['texto_limpio'].apply(lambda t: len(re.findall(r\"\\b\\w+\\b\", str(t)))).sum()\n",
        "\n",
        "print(\"Palabras totales - original:\", int(orig_wc))\n",
        "print(\"Palabras totales - limpio:\", int(clean_wc))\n",
        "\n",
        "def vocab_size(series):\n",
        "    vocab = set()\n",
        "    for txt in series:\n",
        "        for w in str(txt).split():\n",
        "            vocab.add(w)\n",
        "    return len(vocab)\n",
        "\n",
        "print(\"Vocabulario original (aprox.):\", vocab_size(data['texto']))\n",
        "print(\"Vocabulario limpio (aprox.):\", vocab_size(data['texto_limpio']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "518d657b",
      "metadata": {
        "id": "518d657b"
      },
      "source": [
        "## Sección 3: Análisis con BoW/TF‑IDF (25%)\n",
        "\n",
        "- Construcción de matrices documento‑término.\n",
        "- Palabras más frecuentes / distintivas.\n",
        "- Similitud entre documentos (coseno).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b411f7",
      "metadata": {
        "id": "b2b411f7"
      },
      "outputs": [],
      "source": [
        "# Matriz BoW\n",
        "bow_vectorizer = CountVectorizer(max_df=0.9, min_df=1)\n",
        "X_bow = bow_vectorizer.fit_transform(data['texto_limpio'])\n",
        "bow_terms = np.array(bow_vectorizer.get_feature_names_out())\n",
        "print(\"Dimensión BoW:\", X_bow.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7e4d2ea",
      "metadata": {
        "id": "f7e4d2ea"
      },
      "outputs": [],
      "source": [
        "# Palabras más frecuentes globalmente (BoW)\n",
        "term_freq = np.asarray(X_bow.sum(axis=0)).ravel()\n",
        "top_idx = term_freq.argsort()[::-1][:20]\n",
        "pd.DataFrame({'termino': bow_terms[top_idx], 'freq': term_freq[top_idx]}).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8677b038",
      "metadata": {
        "id": "8677b038"
      },
      "outputs": [],
      "source": [
        "# Matriz TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=1)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(data['texto_limpio'])\n",
        "tfidf_terms = np.array(tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"Dimensión TF-IDF:\", X_tfidf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a357c770",
      "metadata": {
        "id": "a357c770"
      },
      "outputs": [],
      "source": [
        "# Términos con mayor TF-IDF promedio\n",
        "tfidf_mean = np.asarray(X_tfidf.mean(axis=0)).ravel()\n",
        "top_tfidf_idx = tfidf_mean.argsort()[::-1][:20]\n",
        "pd.DataFrame({'termino': tfidf_terms[top_tfidf_idx], 'tfidf_prom': tfidf_mean[top_tfidf_idx]}).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff01c5f5",
      "metadata": {
        "id": "ff01c5f5"
      },
      "outputs": [],
      "source": [
        "# Similitud coseno entre documentos (con TF-IDF)\n",
        "sim_matrix = cosine_similarity(X_tfidf)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.imshow(sim_matrix, aspect='auto')\n",
        "plt.title(\"Similitud coseno entre documentos (TF-IDF)\")\n",
        "plt.colorbar()\n",
        "plt.xlabel(\"Documento\")\n",
        "plt.ylabel(\"Documento\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db83bc5f",
      "metadata": {
        "id": "db83bc5f"
      },
      "source": [
        "## Sección 4: Modelado Temático con LDA (20%)\n",
        "\n",
        "Elegí una cantidad de tópicos **k** (por ejemplo, 3 a 8) y justificá. Mostrá las palabras clave por tópico y documentos representativos. Incluí visualización con `pyLDAvis` si es posible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f7dfe86",
      "metadata": {
        "id": "3f7dfe86"
      },
      "outputs": [],
      "source": [
        "# Preparación para LDA en gensim\n",
        "texts_for_lda = [t.split() for t in data['texto_limpio'].tolist()]\n",
        "dictionary = corpora.Dictionary(texts_for_lda)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts_for_lda]\n",
        "\n",
        "k = 5  # <-- ajustar la cantidad de tópicos según tu corpus\n",
        "lda_model = models.LdaModel(corpus=corpus,\n",
        "                            id2word=dictionary,\n",
        "                            num_topics=k,\n",
        "                            random_state=42,\n",
        "                            update_every=1,\n",
        "                            chunksize=100,\n",
        "                            passes=10,\n",
        "                            alpha='auto',\n",
        "                            per_word_topics=True)\n",
        "print(lda_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cb7e0b6",
      "metadata": {
        "id": "8cb7e0b6"
      },
      "outputs": [],
      "source": [
        "# Palabras por tópico\n",
        "topics = lda_model.print_topics(num_topics=k, num_words=10)\n",
        "for tid, t in topics:\n",
        "    print(f\"\\nTópico {tid}: {t}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "310360e0",
      "metadata": {
        "id": "310360e0"
      },
      "outputs": [],
      "source": [
        "# Visualización con pyLDAvis (puede tardar con muchos docs)\n",
        "try:\n",
        "    vis = gensimvis.prepare(lda_model, corpus, dictionary, mds='pcoa')\n",
        "    pyLDAvis.display(vis)\n",
        "except Exception as e:\n",
        "    print(\"No se pudo generar pyLDAvis:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9a42f8f",
      "metadata": {
        "id": "c9a42f8f"
      },
      "source": [
        "## Sección 5: Análisis Avanzado (15%)\n",
        "\n",
        "Elegí **una** alternativa (o más si querés):\n",
        "1) **Embeddings + reducción de dimensión** (LSA/TruncatedSVD) y visualización.  \n",
        "2) **Clustering de documentos** (KMeans) sobre TF‑IDF y caracterización de clusters.  \n",
        "3) **Análisis de sentimiento**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eccf1bf",
      "metadata": {
        "id": "2eccf1bf"
      },
      "outputs": [],
      "source": [
        "# (Opción A) LSA sobre TF-IDF + proyección 2D\n",
        "svd = TruncatedSVD(n_components=2, random_state=42)\n",
        "X_2d = svd.fit_transform(X_tfidf)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.scatter(X_2d[:,0], X_2d[:,1])\n",
        "for i, row in data.iterrows():\n",
        "    plt.text(X_2d[i,0], X_2d[i,1], str(i), fontsize=8)\n",
        "plt.title(\"LSA (SVD) sobre TF-IDF - Proyección 2D\")\n",
        "plt.xlabel(\"Componente 1\")\n",
        "plt.ylabel(\"Componente 2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "148d3e4b",
      "metadata": {
        "id": "148d3e4b"
      },
      "outputs": [],
      "source": [
        "# (Opción B) Clustering KMeans sobre TF-IDF\n",
        "num_clusters = 3  # ajustar según tu corpus\n",
        "km = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)\n",
        "labels = km.fit_predict(X_tfidf)\n",
        "\n",
        "data['cluster'] = labels\n",
        "print(data[['archivo','cluster']].head(10))\n",
        "\n",
        "# Palabras top por cluster (aprox. usando centroides)\n",
        "centroids = km.cluster_centers_\n",
        "terms = tfidf_terms\n",
        "for i in range(num_clusters):\n",
        "    center = centroids[i]\n",
        "    top_idx = center.argsort()[::-1][:10]\n",
        "    print(f\"\\nCluster {i} - términos representativos:\", terms[top_idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6d8f325",
      "metadata": {
        "id": "e6d8f325"
      },
      "source": [
        "## Sección 6: Conclusiones (5%)\n",
        "\n",
        "- ¿Qué patrones relevantes encontraste en el corpus?\n",
        "- ¿Qué técnicas fueron más útiles (BoW/TF‑IDF/LDA/Clustering)?\n",
        "- Limitaciones del corpus y del pipeline\n",
        "- Ideas de mejora (ampliar corpus, etiquetas, fine‑tuning, etc.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9cf0fd9",
      "metadata": {
        "id": "b9cf0fd9"
      },
      "source": [
        "## Sección 7: Presentación y Reproducibilidad (≥10%)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}